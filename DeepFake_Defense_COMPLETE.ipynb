{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6D5-W_8nWdp"
   },
   "source": [
    "# üõ°Ô∏è Defending Against Deepfakes: Texture Feature Perturbation\n",
    "\n",
    "## Based on: \"Defending Deepfake via Texture Feature Perturbation\" (Zhang et al., 2025)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objective\n",
    "**Proactively protect images from deepfake manipulation by injecting invisible \"vaccinations\" into texture regions.**\n",
    "\n",
    "### üí° Key Innovation\n",
    "- Traditional methods: Detect deepfakes AFTER creation\n",
    "- Our approach: PREVENT deepfakes BEFORE creation\n",
    "- How: Add imperceptible perturbations to texture-rich regions\n",
    "\n",
    "### üìä Demo Structure\n",
    "1. ‚úÖ **Clean Image** ‚Üí StarGAN Deepfake ‚Üí **Success** (manipulation works)\n",
    "2. üõ°Ô∏è **Vaccinated Image** ‚Üí StarGAN Deepfake ‚Üí **FAILURE** (manipulation breaks)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq9RkYHvnWdr"
   },
   "source": [
    "## üì¶ Installation & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOxt2ltMnWds",
    "outputId": "158fb86e-93fa-421d-f681-9e60f5c33e73"
   },
   "outputs": [],
   "source": [
    "# Create/reuse dedicated virtual environment and install dependencies\n",
    "# Works on macOS (Apple Silicon), Linux, and Windows.\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import venv\n",
    "from pathlib import Path\n",
    "\n",
    "VENV_DIR = Path('.venv_deepfake')\n",
    "if os.name == 'nt':\n",
    "    VENV_PY = VENV_DIR / 'Scripts' / 'python.exe'\n",
    "else:\n",
    "    VENV_PY = VENV_DIR / 'bin' / 'python'\n",
    "\n",
    "if not VENV_PY.exists():\n",
    "    print(f\"üì¶ Creating virtual environment at {VENV_DIR} ...\")\n",
    "    venv.EnvBuilder(with_pip=True, clear=False).create(VENV_DIR)\n",
    "else:\n",
    "    print(f\"‚úÖ Reusing virtual environment at {VENV_DIR}\")\n",
    "\n",
    "def run(cmd):\n",
    "    print('>', ' '.join(str(x) for x in cmd))\n",
    "    subprocess.check_call([str(x) for x in cmd])\n",
    "\n",
    "packages = [\n",
    "    'torch', 'torchvision', 'torchaudio',\n",
    "    'opencv-python', 'scikit-image', 'matplotlib', 'seaborn',\n",
    "    'pillow', 'numpy', 'scipy', 'tqdm',\n",
    "    'timm', 'gradio==3.50.2', 'gdown', 'requests', 'certifi', 'ipykernel'\n",
    "]\n",
    "\n",
    "marker = VENV_DIR / '.deps_installed_v3'\n",
    "if not marker.exists():\n",
    "    run([VENV_PY, '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools', 'wheel'])\n",
    "    run([VENV_PY, '-m', 'pip', 'install', '-q', *packages])\n",
    "    marker.write_text('ok\\n')\n",
    "    print('‚úÖ Dependencies installed into managed venv.')\n",
    "else:\n",
    "    print('‚úÖ Dependencies already installed (marker found); skipping pip install.')\n",
    "\n",
    "kernel_name = 'deepfake-defense-venv'\n",
    "display_name = 'Python (deepfake-defense-venv)'\n",
    "run([VENV_PY, '-m', 'ipykernel', 'install', '--user', '--name', kernel_name, '--display-name', display_name])\n",
    "\n",
    "current_py = Path(sys.executable).resolve()\n",
    "target_py = Path(VENV_PY).resolve()\n",
    "print(f\"Current kernel python: {current_py}\")\n",
    "print(f\"Target venv python:    {target_py}\")\n",
    "\n",
    "if current_py != target_py:\n",
    "    print(\"\\n‚ö†Ô∏è Switch Jupyter kernel to 'Python (deepfake-defense-venv)' and re-run from the top.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Notebook is already using the managed virtual environment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xc1KBl1DnWdt"
   },
   "source": [
    "## üìö Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIoJYzLnnWdt",
    "outputId": "27afaa3f-e7a1-46d7-84c0-217882b653d8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')  # CPU fallback for unsupported MPS ops\n",
    "\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "# Ensure urllib/torch hub/model downloads use a valid CA bundle.\n",
    "os.environ.setdefault('SSL_CERT_FILE', certifi.where())\n",
    "os.environ.setdefault('REQUESTS_CA_BUNDLE', certifi.where())\n",
    "os.environ.setdefault('CURL_CA_BUNDLE', certifi.where())\n",
    "\n",
    "def _certifi_https_context(*args, **kwargs):\n",
    "    return ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "ssl._create_default_https_context = _certifi_https_context\n",
    "print(f\"üîê SSL CA bundle: {certifi.where()}\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image, ImageFilter\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LPIPS is optional in this notebook path.\n",
    "try:\n",
    "    import lpips\n",
    "    LPIPS_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    lpips = None\n",
    "    LPIPS_AVAILABLE = False\n",
    "    print(f\"‚ö†Ô∏è LPIPS unavailable: {e}\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device setup (CUDA -> MPS -> CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"üöÄ Using device: {device}\")\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(f\"üöÄ Using device: {device}\")\n",
    "    print('   Backend: Apple Metal Performance Shaders (MPS)')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(f\"üöÄ Using device: {device}\")\n",
    "    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_built():\n",
    "        print('   ‚ÑπÔ∏è MPS is built but unavailable (macOS/device/runtime mismatch).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNIEYNTEnWdt"
   },
   "source": [
    "## üîß Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMbk2IAvnWdt",
    "outputId": "de762b73-582d-49cb-c263-94f188beb664"
   },
   "outputs": [],
   "source": [
    "def tensor_to_numpy(tensor):\n",
    "    if len(tensor.shape) == 4:\n",
    "        tensor = tensor[0]\n",
    "    return tensor.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "def calculate_metrics(img1, img2):\n",
    "    from skimage.metrics import structural_similarity as ssim\n",
    "    from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "    if torch.is_tensor(img1):\n",
    "        img1 = tensor_to_numpy(img1)\n",
    "    if torch.is_tensor(img2):\n",
    "        img2 = tensor_to_numpy(img2)\n",
    "\n",
    "    psnr_val = psnr(img1, img2, data_range=1.0)\n",
    "    ssim_val = ssim(img1, img2, channel_axis=2, data_range=1.0)\n",
    "    l2_dist = np.linalg.norm(img1 - img2)\n",
    "\n",
    "    return {'PSNR': psnr_val, 'SSIM': ssim_val, 'L2': l2_dist}\n",
    "\n",
    "def show_cam_on_image(img, mask, use_rgb=True, colormap=cv2.COLORMAP_JET):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    if use_rgb:\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    arr = tensor_to_numpy(tensor)\n",
    "    arr = (arr * 255).clip(0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(arr)\n",
    "\n",
    "def ensure_square_image(pil_img):\n",
    "    if pil_img.width == pil_img.height:\n",
    "        return pil_img, False\n",
    "    side = min(pil_img.width, pil_img.height)\n",
    "    left = (pil_img.width - side) // 2\n",
    "    top = (pil_img.height - side) // 2\n",
    "    square = pil_img.crop((left, top, left + side, top + side))\n",
    "    return square, True\n",
    "\n",
    "# Face localization helpers (OpenCV Haar cascade)\n",
    "FACE_CASCADE = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")\n",
    "\n",
    "def detect_face_bbox(pil_img, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60)):\n",
    "    img_np = np.array(pil_img)\n",
    "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "    faces = FACE_CASCADE.detectMultiScale(gray, scaleFactor=scaleFactor,\n",
    "                                          minNeighbors=minNeighbors, minSize=minSize)\n",
    "    if len(faces) == 0:\n",
    "        return None\n",
    "    return max(faces, key=lambda b: b[2] * b[3])\n",
    "\n",
    "def expand_bbox(bbox, img_w, img_h, margin=0.2):\n",
    "    x, y, w, h = bbox\n",
    "    pad_w = int(w * margin)\n",
    "    pad_h = int(h * margin)\n",
    "    x0 = max(0, x - pad_w)\n",
    "    y0 = max(0, y - pad_h)\n",
    "    x1 = min(img_w, x + w + pad_w)\n",
    "    y1 = min(img_h, y + h + pad_h)\n",
    "    return (x0, y0, x1 - x0, y1 - y0)\n",
    "\n",
    "def crop_face_region(pil_img, margin=0.2, min_size=60):\n",
    "    bbox = detect_face_bbox(pil_img, minSize=(min_size, min_size))\n",
    "    if bbox is None:\n",
    "        return pil_img.copy(), (0, 0, pil_img.width, pil_img.height), False\n",
    "    bbox = expand_bbox(bbox, pil_img.width, pil_img.height, margin)\n",
    "    x, y, w, h = bbox\n",
    "    crop = pil_img.crop((x, y, x + w, y + h))\n",
    "    return crop, bbox, True\n",
    "\n",
    "def composite_on_full(full_pil, crop_tensor, bbox, feather_ratio=0.08):\n",
    "    if full_pil is None or bbox is None:\n",
    "        return None\n",
    "    x, y, w, h = bbox\n",
    "    crop_pil = tensor_to_pil(crop_tensor)\n",
    "    crop_pil = crop_pil.resize((w, h), Image.BICUBIC)\n",
    "\n",
    "    edge = int(max(1, min(w, h) * max(0.0, feather_ratio)))\n",
    "    if edge <= 1:\n",
    "        mask = Image.new('L', (w, h), 255)\n",
    "    else:\n",
    "        inner_w = max(1, w - 2 * edge)\n",
    "        inner_h = max(1, h - 2 * edge)\n",
    "        mask = Image.new('L', (w, h), 0)\n",
    "        mask.paste(Image.new('L', (inner_w, inner_h), 255), (edge, edge))\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=edge * 0.6))\n",
    "\n",
    "    out = full_pil.copy()\n",
    "    out.paste(crop_pil, (x, y), mask)\n",
    "    return out\n",
    "\n",
    "def blend_face_effect(base_pil, deepfake_full_pil, bbox, feather_ratio=0.08):\n",
    "    if base_pil is None or deepfake_full_pil is None:\n",
    "        return None\n",
    "    if bbox is None:\n",
    "        return deepfake_full_pil.copy()\n",
    "\n",
    "    x, y, w, h = bbox\n",
    "    x = max(0, min(x, base_pil.width - 1))\n",
    "    y = max(0, min(y, base_pil.height - 1))\n",
    "    w = max(1, min(w, base_pil.width - x))\n",
    "    h = max(1, min(h, base_pil.height - y))\n",
    "\n",
    "    patch = deepfake_full_pil.crop((x, y, x + w, y + h))\n",
    "\n",
    "    edge = int(max(1, min(w, h) * max(0.0, feather_ratio)))\n",
    "    if edge <= 1:\n",
    "        mask = Image.new('L', (w, h), 255)\n",
    "    else:\n",
    "        inner_w = max(1, w - 2 * edge)\n",
    "        inner_h = max(1, h - 2 * edge)\n",
    "        mask = Image.new('L', (w, h), 0)\n",
    "        mask.paste(Image.new('L', (inner_w, inner_h), 255), (edge, edge))\n",
    "        mask = mask.filter(ImageFilter.GaussianBlur(radius=edge * 0.6))\n",
    "\n",
    "    out = base_pil.copy()\n",
    "    out.paste(patch, (x, y), mask)\n",
    "    return out\n",
    "\n",
    "print(\"‚úÖ Utilities loaded!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dnuo76A1nWdu"
   },
   "source": [
    "## üéØ Grad-CAM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p1E8R5kAnWdu",
    "outputId": "e8ed5ad3-f01b-4fc0-d29d-46e46089a210"
   },
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "        self.target_layer.register_forward_hook(self.save_activation)\n",
    "        self.target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output.detach()\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0].detach()\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        output = self.model(x)\n",
    "        if class_idx is None:\n",
    "            class_idx = output.argmax(dim=1)\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        one_hot = torch.zeros_like(output)\n",
    "        one_hot[0][class_idx] = 1\n",
    "        output.backward(gradient=one_hot, retain_graph=True)\n",
    "\n",
    "        gradients = self.gradients\n",
    "        activations = self.activations\n",
    "        weights = torch.mean(gradients, dim=(2, 3), keepdim=True)\n",
    "        cam = torch.sum(weights * activations, dim=1)\n",
    "        cam = F.relu(cam)\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-8)\n",
    "\n",
    "        return cam.unsqueeze(1)\n",
    "\n",
    "print(\"‚úÖ Grad-CAM implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7pJ7l23nWdu"
   },
   "source": [
    "## üîç Texture Extractor (LBP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zg_ebe-2nWdu",
    "outputId": "5f7a56db-3135-49b9-add6-9d44ccbd118d"
   },
   "outputs": [],
   "source": [
    "class TextureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextureExtractor, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def bilateral_filter(self, img_np, d=31, sigma_color=75, sigma_space=15):\n",
    "        return cv2.bilateralFilter(img_np, d, sigma_color, sigma_space)\n",
    "\n",
    "    def compute_lbp(self, img_np, radius=1, n_points=8):\n",
    "        height, width = img_np.shape\n",
    "        lbp = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "        for i in range(radius, height - radius):\n",
    "            for j in range(radius, width - radius):\n",
    "                center = img_np[i, j]\n",
    "                pattern = 0\n",
    "\n",
    "                for p in range(n_points):\n",
    "                    angle = 2 * np.pi * p / n_points\n",
    "                    x = int(i + radius * np.cos(angle))\n",
    "                    y = int(j - radius * np.sin(angle))\n",
    "                    x = max(0, min(x, height - 1))\n",
    "                    y = max(0, min(y, width - 1))\n",
    "\n",
    "                    if img_np[x, y] >= center:\n",
    "                        pattern += 2 ** p\n",
    "\n",
    "                lbp[i, j] = pattern\n",
    "\n",
    "        return lbp\n",
    "\n",
    "    def forward(self, img_tensor):\n",
    "        batch_size = img_tensor.shape[0]\n",
    "        device = img_tensor.device\n",
    "\n",
    "        texture_features = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            img = img_tensor[b].cpu().numpy()\n",
    "            gray = 0.299 * img[0] + 0.587 * img[1] + 0.116 * img[2]\n",
    "            gray = (gray * 255).astype(np.uint8)\n",
    "            filtered = self.bilateral_filter(gray)\n",
    "            lbp = self.compute_lbp(filtered)\n",
    "            lbp = lbp.astype(np.float32) / 255.0\n",
    "            lbp_tensor = torch.from_numpy(lbp).unsqueeze(0).to(device)\n",
    "            texture_features.append(lbp_tensor)\n",
    "\n",
    "        # FIXED: Keep 4D shape (B, 1, H, W)\n",
    "        lbp_batch = torch.stack(texture_features, dim=0)\n",
    "\n",
    "        x = self.conv1(lbp_batch)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Texture Extractor implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLax8-FjnWdu"
   },
   "source": [
    "## üéØ Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lja8h9sqnWdu",
    "outputId": "c2816ac5-3485-4904-e4d6-302a75a16b0f"
   },
   "outputs": [],
   "source": [
    "class DualAttentionModule:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.resnet = models.resnet50(pretrained=True).to(device).eval()\n",
    "        self.gradcam_resnet = GradCAM(\n",
    "            model=self.resnet,\n",
    "            target_layer=self.resnet.layer4[-1]\n",
    "        )\n",
    "        print(\"‚úÖ Attention Module loaded\")\n",
    "\n",
    "    def get_attention_map(self, img_tensor):\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "\n",
    "        img_normalized = normalize(img_tensor.squeeze(0))\n",
    "        img_input = img_normalized.unsqueeze(0)\n",
    "        img_resized = F.interpolate(img_input, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            cam_tensor = self.gradcam_resnet(img_resized)\n",
    "\n",
    "        original_size = img_tensor.shape[2:]\n",
    "        cam_resized = F.interpolate(cam_tensor, size=original_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        return cam_resized\n",
    "\n",
    "print(\"‚úÖ Attention Module implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vx-4guTZnWdu"
   },
   "source": [
    "## ‚ö° Perturbation Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k-PPlyEBnWdv",
    "outputId": "5a57c393-33af-4aa9-f930-686ba38deb42"
   },
   "outputs": [],
   "source": [
    "class PerturbationEnhancement(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PerturbationEnhancement, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.attention_fusion = nn.Sequential(\n",
    "            nn.Conv2d(129, 64, kernel_size=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 3, kernel_size=3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, texture_features, attention_map):\n",
    "        encoded = self.encoder(texture_features)\n",
    "        attention_downsampled = F.interpolate(\n",
    "            attention_map,\n",
    "            size=encoded.shape[2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "        fused = torch.cat([encoded, attention_downsampled], dim=1)\n",
    "        fused = self.attention_fusion(fused)\n",
    "        perturbation = self.decoder(fused)\n",
    "        return perturbation\n",
    "\n",
    "print(\"‚úÖ Perturbation Generator implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOvKPS5rnWdv"
   },
   "source": [
    "## üî¨ Defense Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXCsrNcnnWdv",
    "outputId": "5cf3db74-78f0-437d-d5ce-8f3d1c33f972"
   },
   "outputs": [],
   "source": [
    "class DeepfakeDefenseFramework(nn.Module):\n",
    "    def __init__(self, epsilon=0.05):\n",
    "        super(DeepfakeDefenseFramework, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.texture_extractor = TextureExtractor()\n",
    "        self.perturbation_gen = PerturbationEnhancement()\n",
    "\n",
    "        # LPIPS is not used in the demo pipeline math, so skip by default.\n",
    "        self.lpips_loss = None\n",
    "        self.lpips_device = None\n",
    "\n",
    "        if LPIPS_AVAILABLE and os.environ.get('ENABLE_LPIPS_INIT', '0') == '1':\n",
    "            try:\n",
    "                self.lpips_loss = lpips.LPIPS(net='alex').to(device)\n",
    "                self.lpips_device = device\n",
    "                print(f\"‚úÖ LPIPS initialized on {self.lpips_device}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è LPIPS init on {device} failed: {e}\")\n",
    "                print('   Retrying LPIPS on CPU...')\n",
    "                try:\n",
    "                    self.lpips_loss = lpips.LPIPS(net='alex').to('cpu')\n",
    "                    self.lpips_device = torch.device('cpu')\n",
    "                    print('‚úÖ LPIPS initialized on CPU')\n",
    "                except Exception as e2:\n",
    "                    print(f\"‚ö†Ô∏è LPIPS disabled (cert/download issue): {e2}\")\n",
    "        else:\n",
    "            print('‚ÑπÔ∏è LPIPS init skipped (not required for this demo).')\n",
    "\n",
    "        print('‚úÖ Defense Framework initialized!')\n",
    "\n",
    "    def generate_perturbation(self, img_tensor, attention_map):\n",
    "        texture_features = self.texture_extractor(img_tensor)\n",
    "        perturbation = self.perturbation_gen(texture_features, attention_map)\n",
    "        perturbation = self.epsilon * perturbation\n",
    "        return perturbation\n",
    "\n",
    "    def vaccinate_image(self, img_tensor, attention_map):\n",
    "        with torch.no_grad():\n",
    "            perturbation = self.generate_perturbation(img_tensor, attention_map)\n",
    "            vaccinated = img_tensor + perturbation\n",
    "            vaccinated = torch.clamp(vaccinated, 0, 1)\n",
    "        return vaccinated, perturbation\n",
    "\n",
    "defense_framework = DeepfakeDefenseFramework(epsilon=0.05).to(device)\n",
    "print(f\"‚úÖ Framework ready on {device}!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHshaon3sdhB",
    "outputId": "e4f9cfda-afe7-40a2-81be-214fb84226a6"
   },
   "outputs": [],
   "source": [
    "import requests, zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR  = Path(\"stargan_celeba_128/models\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ZIP_PATH  = CKPT_DIR / \"celeba-128x128-5attrs.zip\"\n",
    "CKPT_PATH = CKPT_DIR / \"200000-G.ckpt\"\n",
    "\n",
    "if not CKPT_PATH.exists():\n",
    "    DROPBOX_URL = \"https://www.dropbox.com/s/7e966qq0nlxwte4/celeba-128x128-5attrs.zip?dl=1\"\n",
    "    print(\"üì• Downloading StarGAN CelebA-128 weights from Dropbox ‚Ä¶\")\n",
    "    r = requests.get(DROPBOX_URL, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    total = int(r.headers.get(\"content-length\", 0))\n",
    "    with open(ZIP_PATH, \"wb\") as f:\n",
    "        downloaded = 0\n",
    "        for chunk in r.iter_content(chunk_size=1 << 20):\n",
    "            f.write(chunk); downloaded += len(chunk)\n",
    "            print(f\"\\r   {downloaded/1e6:.1f} / {total/1e6:.1f} MB\", end=\"\")\n",
    "    print()\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(CKPT_DIR)\n",
    "    ZIP_PATH.unlink()\n",
    "    print(\"‚úÖ Download & extraction complete.\")\n",
    "else:\n",
    "    print(\"‚úÖ Checkpoint already present, skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XM_wemqCnWdv"
   },
   "source": [
    "## ü§ñ StarGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oP5W6E4nWdv",
    "outputId": "4d12854d-2983-4473-d338-071d2852cc1a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(f\"üîÑ Loading weights from {CKPT_PATH} ‚Ä¶\")\n",
    "\n",
    "# Keep checkpoint load portable across CUDA/MPS/CPU and torch versions.\n",
    "try:\n",
    "    state_dict = torch.load(CKPT_PATH, map_location=\"cpu\", weights_only=True)\n",
    "except TypeError:\n",
    "    state_dict = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "\n",
    "\n",
    "class StarGANGenerator(nn.Module):\n",
    "    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(3 + c_dim, conv_dim, 7, 1, 3, bias=False),\n",
    "            nn.InstanceNorm2d(conv_dim, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        curr_dim = conv_dim\n",
    "        for _ in range(2):\n",
    "            layers += [\n",
    "                nn.Conv2d(curr_dim, curr_dim * 2, 4, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(curr_dim * 2, affine=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            curr_dim *= 2\n",
    "        for _ in range(repeat_num):\n",
    "            layers.append(ResidualBlock(curr_dim, curr_dim))\n",
    "        for _ in range(2):\n",
    "            layers += [\n",
    "                nn.ConvTranspose2d(curr_dim, curr_dim // 2, 4, 2, 1, bias=False),\n",
    "                nn.InstanceNorm2d(curr_dim // 2, affine=True),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            curr_dim //= 2\n",
    "        layers += [nn.Conv2d(curr_dim, 3, 7, 1, 3, bias=False), nn.Tanh()]\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1).expand(c.size(0), c.size(1), x.size(2), x.size(3))\n",
    "        return self.main(torch.cat([x, c], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_9RBHh_suzS",
    "outputId": "c02ab636-f508-4485-c44d-cd416abf9d32"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Reuse the global device selected earlier (cuda / mps / cpu)\n",
    "G = StarGANGenerator(conv_dim=64, c_dim=5, repeat_num=6).to(device)\n",
    "# Strip legacy InstanceNorm running stats (checkpoint saved pre-PyTorch 0.4.0)\n",
    "cleaned = {k: v for k, v in state_dict.items()\n",
    "           if not (k.endswith(\".running_mean\") or k.endswith(\".running_var\"))}\n",
    "\n",
    "G.load_state_dict(cleaned, strict=False)\n",
    "G.eval()\n",
    "print(\"‚úÖ StarGAN Generator weights loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DylBo0OhnWdv"
   },
   "source": [
    "## üì• DOWNLOAD PRE-TRAINED STARGAN WEIGHTS\n",
    "\n",
    "**This is critical! We need trained weights for realistic deepfakes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUxO_JM-nWdv",
    "outputId": "739db559-18fa-49e8-fbcb-3193cd8e3d0a"
   },
   "outputs": [],
   "source": [
    "import os, zipfile, requests, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_DIR  = Path(\"stargan_celeba_128/models\")\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ZIP_PATH  = CKPT_DIR / \"celeba-128x128-5attrs.zip\"\n",
    "CKPT_PATH = CKPT_DIR / \"200000-G.ckpt\"\n",
    "\n",
    "# ‚îÄ‚îÄ 1. Download ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "if not CKPT_PATH.exists():\n",
    "    DROPBOX_URL = \"https://www.dropbox.com/s/7e966qq0nlxwte4/celeba-128x128-5attrs.zip?dl=1\"\n",
    "    print(\"üì• Downloading StarGAN CelebA-128 weights from Dropbox ‚Ä¶\")\n",
    "    r = requests.get(DROPBOX_URL, stream=True, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    total = int(r.headers.get(\"content-length\", 0))\n",
    "    with open(ZIP_PATH, \"wb\") as f:\n",
    "        downloaded = 0\n",
    "        for chunk in r.iter_content(chunk_size=1 << 20):\n",
    "            f.write(chunk); downloaded += len(chunk)\n",
    "            print(f\"\\r   {downloaded/1e6:.1f} / {total/1e6:.1f} MB\", end=\"\")\n",
    "    print()\n",
    "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "        z.extractall(CKPT_DIR)\n",
    "    ZIP_PATH.unlink()\n",
    "    print(\"‚úÖ Extraction complete.\")\n",
    "else:\n",
    "    print(\"‚úÖ Checkpoint already present, skipping download.\")\n",
    "\n",
    "# ‚îÄ‚îÄ 2. Define Generator ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(dim, affine=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1, bias=False),\n",
    "            nn.InstanceNorm2d(dim, affine=True))\n",
    "    def forward(self, x): return x + self.main(x)\n",
    "\n",
    "class StarGANGenerator(nn.Module):\n",
    "    def __init__(self, conv_dim=64, c_dim=5, repeat_num=6):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(3+c_dim, conv_dim, 7, 1, 3, bias=False),\n",
    "                  nn.InstanceNorm2d(conv_dim, affine=True), nn.ReLU(inplace=True)]\n",
    "        curr = conv_dim\n",
    "        for _ in range(2):\n",
    "            layers += [nn.Conv2d(curr, curr*2, 4, 2, 1, bias=False),\n",
    "                       nn.InstanceNorm2d(curr*2, affine=True), nn.ReLU(inplace=True)]\n",
    "            curr *= 2\n",
    "        for _ in range(repeat_num):\n",
    "            layers.append(ResidualBlock(curr))\n",
    "        for _ in range(2):\n",
    "            layers += [nn.ConvTranspose2d(curr, curr//2, 4, 2, 1, bias=False),\n",
    "                       nn.InstanceNorm2d(curr//2, affine=True), nn.ReLU(inplace=True)]\n",
    "            curr //= 2\n",
    "        layers += [nn.Conv2d(curr, 3, 7, 1, 3, bias=False), nn.Tanh()]\n",
    "        self.main = nn.Sequential(*layers)\n",
    "    def forward(self, x, c):\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1).expand(c.size(0), c.size(1), x.size(2), x.size(3))\n",
    "        return self.main(torch.cat([x, c], dim=1))\n",
    "\n",
    "# ‚îÄ‚îÄ 3. Load weights (strip legacy pre-0.4.0 InstanceNorm running stats) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Reuse global device chosen earlier (cuda / mps / cpu)\n",
    "stargan_generator = StarGANGenerator(conv_dim=64, c_dim=5, repeat_num=6).to(device)\n",
    "\n",
    "print(f\"üîÑ Loading weights from {CKPT_PATH} ‚Ä¶\")\n",
    "state_dict = torch.load(CKPT_PATH, map_location=\"cpu\")\n",
    "cleaned    = {k: v for k, v in state_dict.items()\n",
    "              if not (k.endswith(\".running_mean\") or k.endswith(\".running_var\"))}\n",
    "stargan_generator.load_state_dict(cleaned, strict=False)\n",
    "stargan_generator.eval()\n",
    "print(\"‚úÖ StarGAN Generator weights loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QfelH2enWdv"
   },
   "source": [
    "## üì• Load Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hc4wRXHynWdv",
    "outputId": "8452f0e2-6ff4-4e23-81d3-f57520748688"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Create directories\n",
    "# ------------------------------------------------------------------\n",
    "os.makedirs('sample_images', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Use locally downloaded sample images\n",
    "# (make sure these files exist in sample_images/)\n",
    "# ------------------------------------------------------------------\n",
    "sample_image_paths = [\n",
    "    'sample_images/sample_0.jpg',\n",
    "    'sample_images/sample_1.jpg',\n",
    "    'sample_images/sample_2.jpg',\n",
    "]\n",
    "\n",
    "print(\"üìÇ Loading local sample images...\")\n",
    "\n",
    "# Validate files exist\n",
    "valid_paths = []\n",
    "for path in sample_image_paths:\n",
    "    if os.path.exists(path):\n",
    "        valid_paths.append(path)\n",
    "        print(f\"   ‚úÖ Found {path}\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Missing {path}\")\n",
    "\n",
    "if len(valid_paths) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è No local images found. Falling back to synthetic input.\")\n",
    "    test_img = torch.rand(1, 3, 256, 256).to(device)\n",
    "    sample_image_paths = None\n",
    "else:\n",
    "    sample_image_paths = valid_paths\n",
    "    print(f\"\\n‚úÖ {len(sample_image_paths)} sample images ready!\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Image preprocessing\n",
    "# ------------------------------------------------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Face-detection config for display localization only\n",
    "use_face_crop = True\n",
    "face_margin = 0.25  # expand around detected face for effect display\n",
    "min_face_size = 60\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    \"\"\"\n",
    "    Return:\n",
    "      full_pil: square full image used by model and display\n",
    "      full_tensor: model input tensor for full image (defense + attack run here)\n",
    "      bbox: face bbox in full image (used only for effect display)\n",
    "      used_face: whether face detector found a face\n",
    "      square_fixed: whether non-square input was center-cropped to square\n",
    "    \"\"\"\n",
    "    raw_pil = Image.open(image_path).convert('RGB')\n",
    "    full_pil, square_fixed = ensure_square_image(raw_pil)\n",
    "    full_tensor = transform(full_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    if use_face_crop:\n",
    "        _, bbox, used_face = crop_face_region(full_pil, margin=face_margin, min_size=min_face_size)\n",
    "    else:\n",
    "        bbox, used_face = (0, 0, full_pil.width, full_pil.height), False\n",
    "\n",
    "    return full_pil, full_tensor, bbox, used_face, square_fixed\n",
    "\n",
    "print(\"‚úÖ Image loading utilities ready!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxyF9T0CnWdv"
   },
   "source": [
    "## üé¨ Setup Attack Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vf_Br7H7nWdw",
    "outputId": "738c3340-727d-4bb2-cfef-97a610a7d46c"
   },
   "outputs": [],
   "source": [
    "attention_module = DualAttentionModule(device)\n",
    "\n",
    "def deepfake_attack(image_tensor, target_attribute):\n",
    "    \"\"\"\n",
    "    Generate deepfake using StarGAN\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # StarGAN expects [-1, 1] input\n",
    "        img_normalized = image_tensor * 2 - 1\n",
    "\n",
    "        # Generate fake\n",
    "        fake_img = stargan_generator(img_normalized, target_attribute)\n",
    "\n",
    "        # Convert back to [0, 1]\n",
    "        fake_img = (fake_img + 1) / 2\n",
    "        fake_img = torch.clamp(fake_img, 0, 1)\n",
    "\n",
    "    return fake_img\n",
    "\n",
    "# Define attributes (CelebA format)\n",
    "# [Black_Hair, Blond_Hair, Brown_Hair, Male, Young]\n",
    "attributes = {\n",
    "    'Blonde Hair': torch.tensor([[0, 1, 0, 0, 0]], dtype=torch.float32).to(device),\n",
    "    'Old Age': torch.tensor([[0, 0, 0, 0, 0]], dtype=torch.float32).to(device),\n",
    "    'Male': torch.tensor([[0, 0, 0, 1, 0]], dtype=torch.float32).to(device),\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Attack ready!\")\n",
    "print(f\"   Attributes: {list(attributes.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6E9VURJ5nWdw"
   },
   "source": [
    "## üöÄ RUN THE COMPLETE DEMO!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4SMagm6nWdw",
    "outputId": "f334ed4a-a187-4082-b0b6-fac89b6104db"
   },
   "outputs": [],
   "source": [
    "# Load test image\n",
    "if sample_image_paths:\n",
    "    full_pil, full_image_tensor, face_bbox, used_face, square_fixed = prepare_image(sample_image_paths[0])\n",
    "else:\n",
    "    full_image_tensor = torch.rand(1, 3, 256, 256).to(device)\n",
    "    full_pil = tensor_to_pil(full_image_tensor)\n",
    "    face_bbox = (0, 0, full_pil.width, full_pil.height)\n",
    "    used_face = False\n",
    "    square_fixed = False\n",
    "\n",
    "# Defense now runs on the full image tensor.\n",
    "original_image = full_image_tensor\n",
    "\n",
    "target_attr = 'Blonde Hair'\n",
    "target_attr_tensor = attributes[target_attr]\n",
    "\n",
    "print(f\"üéØ Target: {target_attr}\")\n",
    "print(f\"üì∏ Full Tensor: {full_image_tensor.shape}\")\n",
    "if square_fixed:\n",
    "    print(\"‚úÇÔ∏è Input was center-cropped to square before processing.\")\n",
    "if used_face:\n",
    "    print(f\"üë§ Face bbox for deepfake-effect display: {face_bbox}\")\n",
    "else:\n",
    "    print(\"üë§ No face detected; display fallback uses full image.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "50u6GsBsnWdw",
    "outputId": "fcf6c15d-b75c-43c9-ce82-a4d6414af67a"
   },
   "outputs": [],
   "source": [
    "# STEP 1: Attention Map\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: Generating Attention Map\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "attention_map = attention_module.get_attention_map(original_image)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(tensor_to_numpy(original_image))\n",
    "axes[0].set_title('Original (Full)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "attention_np = attention_map[0, 0].cpu().numpy()\n",
    "axes[1].imshow(attention_np, cmap='jet')\n",
    "axes[1].set_title('Attention Heatmap', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "img_np = tensor_to_numpy(original_image)\n",
    "overlay = show_cam_on_image(img_np, attention_np)\n",
    "axes[2].imshow(overlay)\n",
    "axes[2].set_title('Overlay', fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/step1_attention.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Optional: save overlay on full image\n",
    "if full_pil is not None:\n",
    "    overlay_full = Image.fromarray(overlay).resize(full_pil.size, Image.BICUBIC)\n",
    "    overlay_full.save('results/step1_attention_full.png')\n",
    "\n",
    "print(\"‚úÖ Attention map generated!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "DFxd2GiRnWdw",
    "outputId": "766ce159-4a2c-4d4f-fcb2-434797eb8f0e"
   },
   "outputs": [],
   "source": [
    "# STEP 2: Generate Perturbation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: Generating Perturbation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "vaccinated_image, perturbation = defense_framework.vaccinate_image(\n",
    "    original_image,\n",
    "    attention_map\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "axes[0].imshow(tensor_to_numpy(original_image))\n",
    "axes[0].set_title('Original (Full)', fontsize=14, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "pert_amplified = (perturbation * 10).clamp(0, 1)\n",
    "axes[1].imshow(tensor_to_numpy(pert_amplified))\n",
    "axes[1].set_title('Perturbation (10x)', fontsize=14, fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(tensor_to_numpy(vaccinated_image))\n",
    "axes[2].set_title('Vaccinated (Full)', fontsize=14, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "diff = torch.abs(vaccinated_image - original_image) * 20\n",
    "axes[3].imshow(tensor_to_numpy(diff))\n",
    "axes[3].set_title('Difference (20x)', fontsize=14, fontweight='bold')\n",
    "axes[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/step2_perturbation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Vaccinated tensor is full-image model input; also create full-size display PIL.\n",
    "vaccinated_full_tensor = vaccinated_image\n",
    "vaccinated_full_pil = tensor_to_pil(vaccinated_full_tensor).resize(full_pil.size, Image.BICUBIC)\n",
    "vaccinated_full_pil.save('results/vaccinated_full.png')\n",
    "\n",
    "metrics = calculate_metrics(original_image, vaccinated_image)\n",
    "print(\"\\nüìä Full-Image Visual Quality:\")\n",
    "print(f\"   PSNR: {metrics['PSNR']:.2f} dB\")\n",
    "print(f\"   SSIM: {metrics['SSIM']:.4f}\")\n",
    "print(f\"   L2: {metrics['L2']:.4f}\")\n",
    "print(\"\\n‚úÖ Perturbation applied on full image!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqY9_A10nWdw",
    "outputId": "b5789dcb-30b2-4438-df9a-63d1b1300e6c"
   },
   "outputs": [],
   "source": [
    "# STEP 3: Run Attacks\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: Running Deepfake Attacks\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüéØ Attack 1: Clean Full Image ‚Üí StarGAN\")\n",
    "clean_deepfaked = deepfake_attack(full_image_tensor, target_attr_tensor)\n",
    "print(\"   ‚úÖ Deepfake successful\")\n",
    "\n",
    "print(\"\\nüõ°Ô∏è Attack 2: Vaccinated Full Image ‚Üí StarGAN\")\n",
    "vaccinated_deepfaked = deepfake_attack(vaccinated_full_tensor, target_attr_tensor)\n",
    "print(\"   ‚ö†Ô∏è Attempting on protected image...\")\n",
    "\n",
    "# Full-size model outputs\n",
    "clean_deepfaked_full_pil = tensor_to_pil(clean_deepfaked).resize(full_pil.size, Image.BICUBIC)\n",
    "vaccinated_deepfaked_full_pil = tensor_to_pil(vaccinated_deepfaked).resize(full_pil.size, Image.BICUBIC)\n",
    "\n",
    "# Display outputs: only show deepfake effect inside detected face region.\n",
    "clean_effect_display_pil = blend_face_effect(full_pil, clean_deepfaked_full_pil, face_bbox, feather_ratio=0.10)\n",
    "vaccinated_effect_display_pil = blend_face_effect(vaccinated_full_pil, vaccinated_deepfaked_full_pil, face_bbox, feather_ratio=0.10)\n",
    "\n",
    "# Save display outputs\n",
    "clean_effect_display_pil.save('results/deepfake_clean_face_effect.png')\n",
    "vaccinated_effect_display_pil.save('results/deepfake_vaccinated_face_effect.png')\n",
    "\n",
    "defense_metrics = calculate_metrics(clean_deepfaked, vaccinated_deepfaked)\n",
    "print(\"\\nüìä Defense Effectiveness (Full Image Attack):\")\n",
    "print(f\"   L2 Distance: {defense_metrics['L2']:.4f}\")\n",
    "print(f\"   Success: {'YES ‚úÖ' if defense_metrics['L2'] > 0.05 else 'NO ‚ùå'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y1mo55IUnWdw",
    "outputId": "fa99354c-67be-413d-ad82-5c17ed79c105"
   },
   "outputs": [],
   "source": [
    "# STEP 4: FINAL COMPARISON\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: Creating Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "defense_status = \"SUCCESS\" if defense_metrics['L2'] > 0.05 else \"FAILED\"\n",
    "attack_blocked = defense_status == \"SUCCESS\"\n",
    "\n",
    "fig = plt.figure(figsize=(24, 12))\n",
    "gs = fig.add_gridspec(2, 5, hspace=0.3, wspace=0.2)\n",
    "\n",
    "# Row 1: Clean full-image attack (displayed as face-localized effect)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(full_pil)\n",
    "ax1.set_title('1. Original (Full)', fontsize=16, fontweight='bold', color='blue')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.text(0.5, 0.5, '‚Üí\\nStarGAN\\n(No Protection)',\n",
    "         ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax2.axis('off')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.imshow(clean_effect_display_pil)\n",
    "ax3.set_title(f'2. Deepfake Effect\\n(Face Region)', fontsize=16, fontweight='bold', color='red')\n",
    "ax3.axis('off')\n",
    "ax3.text(0.5, -0.1, '‚úÖ ATTACK SUCCESSFUL',\n",
    "         ha='center', transform=ax3.transAxes, fontsize=14,\n",
    "         fontweight='bold', color='red',\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "clean_diff = torch.abs(clean_deepfaked - full_image_tensor)\n",
    "ax4.imshow(tensor_to_numpy(clean_diff))\n",
    "ax4.set_title('Difference (Model Tensor)', fontsize=16, fontweight='bold')\n",
    "ax4.axis('off')\n",
    "\n",
    "ax5 = fig.add_subplot(gs[0, 4])\n",
    "clean_metrics = calculate_metrics(full_image_tensor, clean_deepfaked)\n",
    "metrics_text = f\"\"\"Clean Full-Image Attack:\n",
    "\n",
    "PSNR: {clean_metrics['PSNR']:.2f} dB\n",
    "SSIM: {clean_metrics['SSIM']:.4f}\n",
    "L2: {clean_metrics['L2']:.4f}\n",
    "\n",
    "Result:\n",
    "Natural deepfake\n",
    "\n",
    "‚ùå Vulnerable\n",
    "\"\"\"\n",
    "ax5.text(0.1, 0.5, metrics_text, fontsize=12, family='monospace', va='center')\n",
    "ax5.axis('off')\n",
    "\n",
    "# Row 2: Full-image defense + full-image attack (displayed as face-localized effect)\n",
    "ax6 = fig.add_subplot(gs[1, 0])\n",
    "ax6.imshow(vaccinated_full_pil)\n",
    "ax6.set_title('1. Vaccinated (Full)', fontsize=16, fontweight='bold', color='green')\n",
    "ax6.axis('off')\n",
    "ax6.text(0.5, -0.1, 'üõ°Ô∏è FULL IMAGE PROTECTED',\n",
    "         ha='center', transform=ax6.transAxes, fontsize=14,\n",
    "         fontweight='bold', color='green',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "ax7 = fig.add_subplot(gs[1, 1])\n",
    "ax7.text(0.5, 0.5, '‚Üí\\nStarGAN\\n(Protected Full Image)',\n",
    "         ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "ax7.axis('off')\n",
    "\n",
    "ax8 = fig.add_subplot(gs[1, 2])\n",
    "ax8.imshow(vaccinated_effect_display_pil)\n",
    "ax8_title = '2. Face Effect\\n(FAILED)' if attack_blocked else '2. Face Effect\\n(SUCCEEDED)'\n",
    "ax8_color = 'orange' if attack_blocked else 'red'\n",
    "ax8.set_title(ax8_title, fontsize=16, fontweight='bold', color=ax8_color)\n",
    "ax8.axis('off')\n",
    "status_label = '‚ùå ATTACK FAILED' if attack_blocked else '‚ö†Ô∏è ATTACK SUCCEEDED'\n",
    "status_color = 'green' if attack_blocked else 'red'\n",
    "status_face = 'lightgreen' if attack_blocked else 'mistyrose'\n",
    "ax8.text(0.5, -0.1, status_label,\n",
    "         ha='center', transform=ax8.transAxes, fontsize=14,\n",
    "         fontweight='bold', color=status_color,\n",
    "         bbox=dict(boxstyle='round', facecolor=status_face, alpha=0.6))\n",
    "\n",
    "ax9 = fig.add_subplot(gs[1, 3])\n",
    "vac_diff = torch.abs(vaccinated_deepfaked - vaccinated_full_tensor)\n",
    "ax9.imshow(tensor_to_numpy(vac_diff))\n",
    "ax9.set_title('Difference (Model Tensor)', fontsize=16, fontweight='bold')\n",
    "ax9.axis('off')\n",
    "\n",
    "ax10 = fig.add_subplot(gs[1, 4])\n",
    "vac_metrics = calculate_metrics(vaccinated_full_tensor, vaccinated_deepfaked)\n",
    "metrics_text2 = f\"\"\"Full Defense + Full Attack:\n",
    "\n",
    "PSNR: {vac_metrics['PSNR']:.2f} dB\n",
    "SSIM: {vac_metrics['SSIM']:.4f}\n",
    "L2: {vac_metrics['L2']:.4f}\n",
    "\n",
    "Defense L2: {defense_metrics['L2']:.4f}\n",
    "\n",
    "Result:\n",
    "Visible artifacts\n",
    "\n",
    "‚úÖ Protected: {defense_status}\n",
    "\"\"\"\n",
    "ax10.text(0.1, 0.5, metrics_text2, fontsize=12, family='monospace', va='center')\n",
    "ax10.axis('off')\n",
    "\n",
    "fig.suptitle('Full-Image Vaccination with Face-Localized Deepfake Effect Display',\n",
    "             fontsize=20, fontweight='bold', y=0.98)\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "plt.savefig('results/final_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ DEMO COMPLETE!\")\n",
    "print(f\"   Defense: {defense_status}\")\n",
    "print(f\"   L2: {defense_metrics['L2']:.4f}\")\n",
    "print(\"\\n‚ú® Vaccination is full-image; deepfake effect display is face-localized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhTY2Ne1nWdw"
   },
   "source": [
    "## üöÄ Interactive Gradio Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VuR2iYjKnWdw",
    "outputId": "8b945bb2-033d-478c-d895-fe07008b34d4"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gradio as gr\n",
    "except ModuleNotFoundError:\n",
    "    raise RuntimeError(\n",
    "        \"Gradio is not installed in this kernel. Run the setup cell, switch to \"\n",
    "        \"'Python (deepfake-defense-venv)', then run again.\"\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Gradio {gr.__version__} ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "DOo1XNopuFtP",
    "outputId": "d7158137-6511-4cca-83fd-3d052141f61e"
   },
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Core function ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def interactive_defense(image, attribute_choice, epsilon_val, face_margin_val, blend_feather_val):\n",
    "    try:\n",
    "        full_pil = image.convert('RGB')\n",
    "        full_pil, square_fixed = ensure_square_image(full_pil)\n",
    "\n",
    "        full_tensor = transform(full_pil).unsqueeze(0).to(device)\n",
    "        _, face_bbox, used_face = crop_face_region(full_pil, margin=face_margin_val, min_size=min_face_size)\n",
    "\n",
    "        # Vaccinate the full image (not ROI-only).\n",
    "        attention_map = attention_module.get_attention_map(full_tensor)\n",
    "        defense_framework.epsilon = epsilon_val\n",
    "        vaccinated_full_tensor, _ = defense_framework.vaccinate_image(full_tensor, attention_map)\n",
    "        vaccinated_full_pil = tensor_to_pil(vaccinated_full_tensor).resize(full_pil.size, Image.BICUBIC)\n",
    "\n",
    "        target_attr = attributes[attribute_choice]\n",
    "        clean_fake_full = deepfake_attack(full_tensor, target_attr)\n",
    "        vac_fake_full = deepfake_attack(vaccinated_full_tensor, target_attr)\n",
    "\n",
    "        clean_fake_full_pil = tensor_to_pil(clean_fake_full).resize(full_pil.size, Image.BICUBIC)\n",
    "        vac_fake_full_pil = tensor_to_pil(vac_fake_full).resize(full_pil.size, Image.BICUBIC)\n",
    "\n",
    "        # Display deepfake effect only in the detected face region.\n",
    "        out_clean = blend_face_effect(full_pil, clean_fake_full_pil, face_bbox, feather_ratio=blend_feather_val)\n",
    "        out_vac_f = blend_face_effect(vaccinated_full_pil, vac_fake_full_pil, face_bbox, feather_ratio=blend_feather_val)\n",
    "\n",
    "        def_metric = calculate_metrics(clean_fake_full, vac_fake_full)\n",
    "        vis_metric = calculate_metrics(full_tensor, vaccinated_full_tensor)\n",
    "        success = \"‚úÖ SUCCESS\" if def_metric['L2'] > 0.05 else \"‚ùå FAILED\"\n",
    "\n",
    "        bbox_text = f\"face_bbox={face_bbox}\" if used_face else \"face_bbox=full_image_fallback\"\n",
    "        square_text = \"square_center_crop_applied\" if square_fixed else \"already_square\"\n",
    "\n",
    "        metrics_text = (\n",
    "            f\"üìä RESULTS:\\n\\n\"\n",
    "            f\"Defense: {success}\\n\"\n",
    "            f\"L2 Distance: {def_metric['L2']:.4f}\\n\\n\"\n",
    "            f\"Quality (Full Image):\\n\"\n",
    "            f\"  PSNR: {vis_metric['PSNR']:.2f} dB\\n\"\n",
    "            f\"  SSIM: {vis_metric['SSIM']:.4f}\\n\\n\"\n",
    "            f\"Display ROI: {bbox_text}\\n\"\n",
    "            f\"Input: {square_text}\\n\"\n",
    "            f\"Face Margin (display): {face_margin_val:.2f}\\n\"\n",
    "            f\"Blend Feather (display): {blend_feather_val:.2f}\"\n",
    "        )\n",
    "\n",
    "        return full_pil, vaccinated_full_pil, out_clean, out_vac_f, metrics_text\n",
    "\n",
    "    except Exception:\n",
    "        import traceback\n",
    "        return None, None, None, None, f\"‚ùå Error:\\n{traceback.format_exc()}\"\n",
    "\n",
    "# ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with gr.Blocks(theme=gr.themes.Soft(), title=\"üõ°Ô∏è Deepfake Defense\") as demo:\n",
    "    gr.Markdown(\"# üõ°Ô∏è Deepfake Defense: Texture-Aware Protection\")\n",
    "    gr.Markdown(\"Vaccination runs on the full image. Face detection is used only to localize the displayed deepfake effect.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            inp_image = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
    "            inp_attr = gr.Dropdown(list(attributes.keys()), value=\"Blonde Hair\", label=\"Attack Type\")\n",
    "            inp_epsilon = gr.Slider(0.01, 0.1, value=0.05, step=0.01, label=\"Epsilon\")\n",
    "            inp_face_margin = gr.Slider(0.10, 0.60, value=0.30, step=0.01, label=\"Face Margin (Display ROI)\")\n",
    "            inp_blend_feather = gr.Slider(0.00, 0.25, value=0.10, step=0.01, label=\"Blend Feather (Display ROI)\")\n",
    "            run_btn = gr.Button(\"üöÄ Run Defense\", variant=\"primary\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Row():\n",
    "                out_orig = gr.Image(label=\"Original (Full)\")\n",
    "                out_vacc = gr.Image(label=\"Vaccinated (Full)\")\n",
    "            with gr.Row():\n",
    "                out_clean = gr.Image(label=\"Deepfake Effect on Original (Face Region)\")\n",
    "                out_vac_f = gr.Image(label=\"Deepfake Effect on Vaccinated (Face Region)\")\n",
    "            out_metrics = gr.Textbox(label=\"Metrics\", lines=10)\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=interactive_defense,\n",
    "        inputs=[inp_image, inp_attr, inp_epsilon, inp_face_margin, inp_blend_feather],\n",
    "        outputs=[out_orig, out_vacc, out_clean, out_vac_f, out_metrics]\n",
    "    )\n",
    "\n",
    "print(\"üöÄ Launching Gradio...\")\n",
    "demo.launch(share=True, debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv_deepfake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
